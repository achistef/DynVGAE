{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sps\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "snapshot_prefix = \"C:/Users/Achil/Documents/VGAE/src/data/hive/hive-node-snapshot/hive-node-snapshot-\"\n",
    "\n",
    "num_nodes = 386\n",
    "num_edges = 23459#61125\n",
    "num_features = num_nodes\n",
    "num_snapshots = 10\n",
    "snapshot_nodes = int(num_nodes / num_snapshots)\n",
    "intermediate_size = 32\n",
    "embedding_size = 16\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "l_depth = 1\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def make_test_matrix(coords, values, shape):\n",
    "    rows = np.concatenate((coords[:, 0], coords[:, 1]))\n",
    "    cols = np.concatenate((coords[:, 1], coords[:, 0]))\n",
    "    vals = np.concatenate((values, values))\n",
    "    return sps.coo_matrix((vals, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "def split_features(features, snapshot_id):\n",
    "    rows = (snapshot_id + 1) * snapshot_nodes\n",
    "    split = features[:rows,:]\n",
    "    coo = split.tocoo().astype(np.float32)\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    features_sparse_tensor = tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    return features_sparse_tensor\n",
    "\n",
    "def load_features_identity():\n",
    "    # Identity matrix\n",
    "    features = sps.identity(num_nodes, dtype=np.float32, format='csr')\n",
    "    return features\n",
    "\n",
    "def load_features(features_filename):\n",
    "    csr = sps.load_npz(features_filename)\n",
    "    return csr\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    sparse_mx = sps.triu(sparse_mx)\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sample_coords(matrix, sample_size):\n",
    "    matrix = sps.coo_matrix(matrix)\n",
    "    coords, values, shape = sparse_to_tuple(matrix)\n",
    "    perm = np.random.rand(coords.shape[0]).argsort()\n",
    "    np.take(coords, perm, axis=0, out=coords)\n",
    "    np.take(values, perm, axis=0, out=values)\n",
    "    return coords[:sample_size], values[:sample_size]\n",
    "\n",
    "def sample_edges(adj, sample_ratio):\n",
    "    sample_size = int(sample_ratio * adj.count_nonzero() / 2)\n",
    "    pos_edges_coords, pos_edges_values = sample_coords(adj, sample_size)\n",
    "    t_matrix = sp.full(adj.shape, 1)\n",
    "    neg_adj = t_matrix - adj - np.eye(adj.shape[0])\n",
    "    neg_adj = sp.sparse.coo_matrix(neg_adj)\n",
    "    neg_edges_coords, zeros = sample_coords(neg_adj, sample_size)\n",
    "    return pos_edges_coords, pos_edges_values, neg_edges_coords\n",
    "    \n",
    "def calc_normalized(adj_):\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sps.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo().astype(np.float32)\n",
    "    return adj_normalized\n",
    "\n",
    "def adj_from_snapshot(snapshot_id):\n",
    "    npy_adj = np.load(snapshot_prefix + str(snapshot_id) + '.npy')\n",
    "    csr_adj = sps.csr_matrix(npy_adj)\n",
    "    return csr_adj\n",
    "\n",
    "def load_training_snapshot(snapshot_id):\n",
    "    adj = adj_from_snapshot(snapshot_id)\n",
    "    test_pos_edges, test_edges_values, test_neg_edges = sample_edges(adj, test_ratio)\n",
    "    adj_test = make_test_matrix(test_pos_edges, test_edges_values, adj.shape)\n",
    "    \n",
    "    adj = adj - adj_test\n",
    "    adj.eliminate_zeros()\n",
    "    train_pos_edges, train_edges_values, train_neg_edges = sample_edges(adj, 1)\n",
    "    \n",
    "    adj_with_diag = adj + sps.identity(adj.shape[0], dtype=np.float32).tocsr()\n",
    "\n",
    "    adj_tensor = tf.Variable(adj_with_diag.todense(), dtype=tf.float32)\n",
    "\n",
    "    adj_norm = calc_normalized(adj_with_diag)\n",
    "    indices = np.mat([adj_norm.row, adj_norm.col]).transpose()\n",
    "    adj_norm_tensor = tf.SparseTensor(indices, adj_norm.data, adj_norm.shape)\n",
    "\n",
    "    return adj_tensor, adj_norm_tensor, test_pos_edges, test_edges_values, test_neg_edges, train_pos_edges, test_edges_values, train_neg_edges\n",
    "\n",
    "def auc(pe, ne, embeddings):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, coords in enumerate(pe):\n",
    "        emb1 = embeddings[coords[0]]\n",
    "        emb2 = embeddings[coords[1]]\n",
    "        pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "        y_true.append(1)\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    for coords in ne:\n",
    "        emb1 = embeddings[coords[0]]\n",
    "        emb2 = embeddings[coords[1]]\n",
    "        pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "        y_true.append(0)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true=y_true, y_score=y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "features = load_features_identity()\n",
    "adj_snapshots = []\n",
    "adj_norm_snapshots = []\n",
    "features_snapshots = []\n",
    "test_pos_edges_snapshot = []\n",
    "test_edges_values_snapshot = []\n",
    "test_neg_edges_snapshot = []\n",
    "num_nodes_snapshot = []\n",
    "train_pos_edges_snapshot = []\n",
    "train_neg_edges_snapshot = []\n",
    "train_edges_values_snapshot = []\n",
    "pos_edges_combined = []\n",
    "neg_edges_combined = []\n",
    "pos_edges_incr = []\n",
    "neg_edges_incr = []\n",
    "\n",
    "for i in range(num_snapshots):\n",
    "    snapshot_adj, snapshot_adj_norm, test_pos_edges, test_edges_values, test_neg_edges, train_pos_edges, train_edges_values, train_neg_edges = load_training_snapshot(i)\n",
    "    snapshot_features = split_features(features, i)\n",
    "    adj_snapshots.append(snapshot_adj)\n",
    "    adj_norm_snapshots.append(snapshot_adj_norm)\n",
    "    features_snapshots.append(snapshot_features)\n",
    "    test_pos_edges_snapshot.append(test_pos_edges)\n",
    "    test_edges_values_snapshot.append(test_edges_values)\n",
    "    test_neg_edges_snapshot.append(test_neg_edges)\n",
    "    train_pos_edges_snapshot.append(train_pos_edges)\n",
    "    train_edges_values_snapshot.append(train_edges_values)\n",
    "    train_neg_edges_snapshot.append(train_neg_edges)\n",
    "    pos_edges_combined.append(np.concatenate((test_pos_edges, train_pos_edges), axis=0))\n",
    "    neg_edges_combined.append(np.concatenate((test_neg_edges, train_neg_edges), axis=0))\n",
    "    if i > 0:\n",
    "        pos_inc = list(set(map(tuple, pos_edges_combined[i])) - set(map(tuple, pos_edges_combined[i-1])))\n",
    "        neg_inc = list(set(map(tuple, neg_edges_combined[i])) - set(map(tuple, neg_edges_combined[i-1])))[:len(pos_inc)]\n",
    "        \n",
    "        pos_edges_incr.append(pos_inc)\n",
    "        neg_edges_incr.append(neg_inc)\n",
    "    \n",
    "    num_nodes_snapshot.append(snapshot_adj.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, adj_norm, shared_w0):\n",
    "        super(FirstLayer, self).__init__()\n",
    "        self.adj_norm = adj_norm\n",
    "        self.w = shared_w0\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        xw = tf.sparse.sparse_dense_matmul(inputs, self.w)\n",
    "        axw = tf.sparse.sparse_dense_matmul(self.adj_norm, xw)\n",
    "        relu = tf.nn.relu(axw)\n",
    "        return relu\n",
    "    \n",
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, adj_norm):\n",
    "        super(SecondLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.adj_norm = adj_norm\n",
    "        self.training = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                initializer=tf.keras.initializers.glorot_uniform(),\n",
    "                                trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = tf.matmul(inputs, self.w)\n",
    "        x = tf.sparse.sparse_dense_matmul(self.adj_norm, x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, adj_norm, embedding_size, shared_w0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.first_layer = FirstLayer(adj_norm, shared_w0)\n",
    "        self.mean_layer = SecondLayer(embedding_size, adj_norm)\n",
    "        self.std_layer = SecondLayer(embedding_size, adj_norm)\n",
    "    \n",
    "    def call(self, input_features, **kwargs):\n",
    "        intermediate = self.first_layer(input_features)\n",
    "        means = self.mean_layer(intermediate)\n",
    "        stds = self.std_layer(intermediate)\n",
    "        z = means + (tf.random.normal(shape=means.shape) * tf.exp(stds))\n",
    "        return z, means, stds\n",
    "    \n",
    "class ThirdLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ThirdLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        matmul = tf.matmul(inputs, inputs, transpose_b=True)\n",
    "        flat = tf.reshape(matmul, [-1])\n",
    "        return flat\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.third_layer = ThirdLayer()\n",
    "    \n",
    "    def call(self, input_features, **kwargs):\n",
    "        return self.third_layer(input_features)\n",
    "    \n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, adj_norm, intermediate_size, embedding_size, shared_w0):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(adj_norm, embedding_size, shared_w0)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    \n",
    "    def call(self, input_features, **kwargs):\n",
    "        z, means, stds = self.encoder(input_features)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, means, stds    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "glorot_initializer = tf.keras.initializers.glorot_uniform()\n",
    "shared_w0 = tf.Variable(initial_value=glorot_initializer(shape=(num_features, intermediate_size), dtype=tf.float32), trainable=True)\n",
    "\n",
    "autoencoders = []\n",
    "pos_weights = []\n",
    "norms = []\n",
    "labels = []\n",
    "\n",
    "for i in range(num_snapshots):\n",
    "    adj = adj_snapshots[i]\n",
    "    adj_norm = adj_norm_snapshots[i]\n",
    "    \n",
    "    autoencoders.append(Autoencoder(adj_norm, intermediate_size, embedding_size, shared_w0))\n",
    "    adj_sum = tf.reduce_sum(adj)\n",
    "    pos_weights.append(float((adj.shape[0] * adj.shape[0]) - adj_sum) / adj_sum)\n",
    "    norms.append(adj.shape[0] * adj.shape[0] / float(((adj.shape[0] * adj.shape[0]) - adj_sum) * 2))\n",
    "    labels.append(tf.reshape(adj, [-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start training\n",
      "step 0\n",
      "epoch: 0 loss 1.485008 exec time 0.009932994842529297\n",
      "epoch: 10 loss 1.4007387 exec time 0.001994609832763672\n",
      "epoch: 20 loss 0.9578406 exec time 0.001996755599975586\n",
      "epoch: 30 loss 0.8286164 exec time 0.006980419158935547\n",
      "epoch: 40 loss 0.7545888 exec time 0.0069828033447265625\n",
      "epoch: 50 loss 0.7343683 exec time 0.0030362606048583984\n",
      "epoch: 60 loss 0.677586 exec time 0.002039194107055664\n",
      "epoch: 70 loss 0.72615194 exec time 0.002039194107055664\n",
      "epoch: 80 loss 0.70412576 exec time 0.0029931068420410156\n",
      "epoch: 90 loss 0.6665158 exec time 0.0049877166748046875\n",
      "step 1\n",
      "auc score 0.7881862381829835\n",
      "l 0\n",
      "epoch: 0 loss 1.7340312 exec time 0.010970115661621094\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 10 loss 1.0054775 exec time 0.00494837760925293\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 20 loss 0.6765732 exec time 0.003983497619628906\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 30 loss 0.64786 exec time 0.004029750823974609\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 40 loss 0.61781585 exec time 0.004052162170410156\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 50 loss 0.60392034 exec time 0.005012989044189453\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 60 loss 0.5894904 exec time 0.004004478454589844\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 70 loss 0.58863086 exec time 0.015957117080688477\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 80 loss 0.5730743 exec time 0.00399017333984375\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "epoch: 90 loss 0.57926023 exec time 0.005938291549682617\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "l 0\n",
      "step 2\n",
      "auc score 0.8514639102090054\n",
      "l 1\n",
      "epoch: 0 loss 1.9616365 exec time 0.010036706924438477\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 10 loss 0.718143 exec time 0.005035400390625\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 20 loss 0.63058186 exec time 0.005639314651489258\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 30 loss 0.60443544 exec time 0.006022214889526367\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 40 loss 0.5805508 exec time 0.007041215896606445\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 50 loss 0.5650674 exec time 0.0059850215911865234\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 60 loss 0.5593461 exec time 0.004987955093383789\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 70 loss 0.5604261 exec time 0.007977724075317383\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 80 loss 0.5523428 exec time 0.006983518600463867\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "epoch: 90 loss 0.5564956 exec time 0.005010128021240234\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "l 1\n",
      "step 3\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7cf4dc226935>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlast_trained_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mauc_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_edges_incr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_edges_incr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mauc_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auc score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-6759974fad94>\u001b[0m in \u001b[0;36mauc\u001b[1;34m(pe, ne, embeddings)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0memb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0memb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGAE\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes, name)\u001b[0m\n\u001b[0;32m   4075\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[0;32m   4076\u001b[0m         b, b_axes, True)\n\u001b[1;32m-> 4077\u001b[1;33m     \u001b[0mab_matmul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4078\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mab_matmul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_free_dims\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGAE\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGAE\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2763\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2765\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGAE\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6111\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6112\u001b[1;33m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[0;32m   6113\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6114\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "print(\"start training\")\n",
    "snapshot_history = defaultdict(list)\n",
    "kl_loss_history = defaultdict(list)\n",
    "reconstructed_loss_history = defaultdict(list)\n",
    "auc_history = []\n",
    "start_global = time.time()\n",
    "\n",
    "kl_losses = {}\n",
    "for i in range(num_snapshots):\n",
    "    print('step', i)\n",
    "    if i > 0:\n",
    "        last_trained_ae = autoencoders[i-1]\n",
    "        test_adj_norm = adj_norm_snapshots[i]\n",
    "        test_features = features_snapshots[i]\n",
    "        last_trained_ae.encoder.first_layer.adj_norm = test_adj_norm\n",
    "        last_trained_ae.encoder.mean_layer.adj_norm = test_adj_norm\n",
    "        last_trained_ae.encoder.std_layer.adj_norm = test_adj_norm\n",
    "        r, embeddings, s  = last_trained_ae(test_features)\n",
    "        \n",
    "        auc_score = auc(pos_edges_incr[i-1], neg_edges_incr[i-1], embeddings)\n",
    "        auc_history.append(auc_score)\n",
    "        print('auc score', auc_score)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            autoenc = autoencoders[i]\n",
    "            reconstructed, means, stds = autoenc(features_snapshots[i])\n",
    "            reconstruction_loss = norms[i] * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=reconstructed, labels=labels[i], pos_weight=pos_weights[i]))\n",
    "            kl_self_loss = tf.abs((0.5 / num_nodes_snapshot[i]) * tf.reduce_mean(tf.reduce_sum(1 + 2 * stds - tf.square(means) - tf.square(tf.exp(stds)), 1)))\n",
    "            kl_loss = 0\n",
    "            if i == 0:\n",
    "                kl_loss += kl_self_loss\n",
    "            else:\n",
    "                for l in range(i-1, max(-1, i -1 - l_depth), -1):\n",
    "                    prev_kl = kl_losses[l]\n",
    "                    kl_loss += (kl_self_loss + prev_kl) / 2\n",
    "            kl_losses[i] = kl_loss\n",
    "\n",
    "            step_loss = reconstruction_loss + kl_loss\n",
    "            snapshot_history[i].append(step_loss)\n",
    "            kl_loss_history[i].append(kl_loss)\n",
    "            reconstructed_loss_history[i].append(reconstruction_loss)\n",
    "            \n",
    "            # clear_output()\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch:', epoch, 'loss', step_loss.numpy(), 'exec time', time.time() - start)\n",
    "            \n",
    "        gradients = tape.gradient(step_loss, autoenc.trainable_variables)\n",
    "        gradient_variables = zip(gradients, autoenc.trainable_variables)\n",
    "        opt.apply_gradients(gradient_variables)\n",
    "\n",
    "total = time.time() - start_global\n",
    "print(\"elapsed: \" + str(total) + \" seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "x_axis = range(epochs)\n",
    "plt.figure()\n",
    "for i in range(num_snapshots-1):\n",
    "    plt.plot(x_axis, snapshot_history[i], label=str(i))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss per autoencoder')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('Total loss during training for each autoencoder')\n",
    "\n",
    "\n",
    "plt.figure() \n",
    "for i in range(num_snapshots-1):\n",
    "    plt.plot(x_axis, reconstructed_loss_history[i], label=str(i))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Loss per autoencoder')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('Reconstruction loss during training')\n",
    "\n",
    "plt.figure()\n",
    "for i in range(num_snapshots-1):\n",
    "    plt.plot(x_axis, kl_loss_history[i], label=str(i))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL Divergence Loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('KL Divergence during training')\n",
    "\n",
    "x_axis = range(num_snapshots-1)\n",
    "plt.figure()\n",
    "plt.plot(x_axis, auc_history)\n",
    "plt.xlabel('Snapshot')\n",
    "plt.ylabel('AUC')\n",
    "plt.title(\"AUC during training for trainset\")\n",
    "\n",
    "y_axis = [len(e) for e in pos_edges_incr]\n",
    "plt.figure()\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.xlabel('Snapshot')\n",
    "plt.ylabel('Number of new edges')\n",
    "plt.title(\"New edges in each snapshot\")\n",
    "\n",
    "y_axis = [len(e)/10000 for e in pos_edges_incr]\n",
    "plt.figure()\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.plot(x_axis, auc_history)\n",
    "plt.xlabel('Snapshot')\n",
    "plt.ylabel('Number of new edges')\n",
    "plt.title(\"New edges in each snapshot\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# link prediction\n",
    "# last_trained_ae = autoencoders[num_snapshots-2]\n",
    "# test_adj_norm = adj_norm_snapshots[num_snapshots-1]\n",
    "# test_features = features_snapshots[num_snapshots-1]\n",
    "# last_trained_ae.encoder.first_layer.adj_norm = test_adj_norm\n",
    "# last_trained_ae.encoder.mean_layer.adj_norm = test_adj_norm\n",
    "# last_trained_ae.encoder.std_layer.adj_norm = test_adj_norm\n",
    "# \n",
    "# reconstructed, embeddings, stds  = last_trained_ae(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# last_trained_ae = autoencoders[num_snapshots-2]\n",
    "# test_adj_norm = adj_norm_snapshots[num_snapshots-2]\n",
    "# test_features = features_snapshots[num_snapshots-2]\n",
    "# last_trained_ae.encoder.first_layer.adj_norm = test_adj_norm\n",
    "# last_trained_ae.encoder.mean_layer.adj_norm = test_adj_norm\n",
    "# last_trained_ae.encoder.std_layer.adj_norm = test_adj_norm\n",
    "# \n",
    "# reconstructed, embeddings, stds  = last_trained_ae(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def total_auc(pe, ne, embeddings):\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     \n",
    "#     for pos_edges in pe:\n",
    "#         for coords in pos_edges:\n",
    "#             # if coords[0] > 342 or coords[1] > 342:\n",
    "#                 emb1 = embeddings[coords[0]]\n",
    "#                 emb2 = embeddings[coords[1]]\n",
    "#                 pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "#                 y_true.append(1)\n",
    "#                 y_pred.append(pred)\n",
    "#     \n",
    "#     for neg_edges in ne:\n",
    "#         for coords in neg_edges:\n",
    "#             # if coords[0] > 342 or coords[1] > 342:\n",
    "#                 emb1 = embeddings[coords[0]]\n",
    "#                 emb2 = embeddings[coords[1]]\n",
    "#                 pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "#                 y_true.append(0)\n",
    "#                 y_pred.append(pred)\n",
    "# \n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y_true=y_true, y_score=y_pred)\n",
    "#     roc_auc = metrics.auc(fpr, tpr)\n",
    "#     print(\"ROC score:\")\n",
    "#     print(roc_auc)\n",
    "#     plt.figure()\n",
    "#     lw = 2\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('ROC')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "# \n",
    "# \n",
    "# total_auc(test_pos_edges_snapshot[:], test_neg_edges_snapshot[:], embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def mse(pe, pv, ne, embeddings):\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     \n",
    "#     for i, pos_edges in enumerate(pe):\n",
    "#         for j, coords in enumerate(pos_edges):\n",
    "#             # if coords[0] > 342 or coords[1] > 342:\n",
    "#                 emb1 = embeddings[coords[0]]\n",
    "#                 emb2 = embeddings[coords[1]]\n",
    "#                 pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "#                 y_true.append(pv[i][j])\n",
    "#                 y_pred.append(pred)\n",
    "#     \n",
    "#     for neg_edges in ne:\n",
    "#         for coords in neg_edges:\n",
    "#             # if coords[0] > 342 or coords[1] > 342:\n",
    "#                 emb1 = embeddings[coords[0]]\n",
    "#                 emb2 = embeddings[coords[1]]\n",
    "#                 pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()\n",
    "#                 y_true.append(0)\n",
    "#                 y_pred.append(pred)\n",
    "#     \n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "#     mse = ((y_true - y_pred)**2).mean()\n",
    "#     return mse\n",
    "# \n",
    "# er = mse(test_pos_edges_snapshot[:], test_edges_values_snapshot[:], test_neg_edges_snapshot[:], embeddings)\n",
    "# print(er)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-1b1c1077",
   "language": "python",
   "display_name": "PyCharm (VGAE)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}