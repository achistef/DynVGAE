#%%

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

import time
import numpy as np
import scipy as sp
import networkx as nx
import tensorflow as tf
import scipy.sparse as sps
from sklearn import metrics
from numpy import genfromtxt
import matplotlib.pyplot as plt
from collections import defaultdict
from IPython.display import clear_output

snapshot_prefix = "C:/Users/Achil/Documents/VGAE/src/data/hive/hive-edgelist-snapshot/edges"

num_snapshots = 30
intermediate_size = 32
embedding_size = 16
epochs = 100
learning_rate = 0.01
l_depth = 1
test_ratio = 0.8
edge_weight_thres = 0

#%%

def matrix_from(coords, values, shape):
    rows = np.concatenate((coords[:, 0], coords[:, 1]))
    cols = np.concatenate((coords[:, 1], coords[:, 0]))
    vals = np.concatenate((values, values))
    return sps.coo_matrix((vals, (rows, cols)), shape=shape).tocsr()

def sparse_to_tuple(sparse_mx):
    sparse_mx = sps.triu(sparse_mx)
    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()
    values = sparse_mx.data
    shape = sparse_mx.shape
    return coords, values, shape

def sample_coords(matrix, sample_size):
    matrix = sps.coo_matrix(matrix)
    coords, values, shape = sparse_to_tuple(matrix)
    perm = np.random.rand(coords.shape[0]).argsort()
    np.take(coords, perm, axis=0, out=coords)
    np.take(values, perm, axis=0, out=values)
    return coords[:sample_size], values[:sample_size]

def sample_edges(adj_full, adj_inc, sample_ratio):
    sample_size = int(sample_ratio * adj_inc.count_nonzero() / 2)
    pos_edges_coords, pos_edges_values = sample_coords(adj_inc, sample_size)
    t_matrix = sp.full(adj_full.shape, 1)
    neg_adj = t_matrix - adj_full - np.eye(adj_full.shape[0])
    neg_adj = np.floor(neg_adj)
    neg_adj = sp.sparse.coo_matrix(neg_adj)
    neg_edges_coords, zeros = sample_coords(neg_adj, sample_size)
    return pos_edges_coords, pos_edges_values, neg_edges_coords

def sample_by_size(pos_edges, neg_edges, node_limit):
    pe = [e for e in pos_edges if e[0] < node_limit and e[1] < node_limit]
    ne = [e for e in neg_edges if e[0] < node_limit and e[1] < node_limit][:len(pe)]
    return pe, ne
    
def calc_normalized(adj_):
    rowsum = np.array(adj_.sum(1))
    degree_mat_inv_sqrt = sps.diags(np.power(rowsum, -0.5).flatten())
    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo().astype(np.float32)
    return adj_normalized

def load_edgelist(snapshot_id):
    snapshot_dir = snapshot_prefix + str((snapshot_id+1)*10) + '.csv'
    edgelist = genfromtxt(snapshot_dir, delimiter=',', skip_header=0)
    edgelist = edgelist[edgelist[:,2] > edge_weight_thres]
    return edgelist

def auc(pe, ne, embeddings):
    # too few edges, cannot compute AUC
    if len(pe) == 0:
        return 0
    y_true = []
    y_pred = []
    
    for coords in pe:
        emb1 = embeddings[coords[0]]
        emb2 = embeddings[coords[1]]
        pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()
        y_true.append(1)
        y_pred.append(pred)
    
    for coords in ne:
        emb1 = embeddings[coords[0]]
        emb2 = embeddings[coords[1]]
        pred = tf.sigmoid(tf.tensordot(emb1, emb2, 1)).numpy()
        y_true.append(0)
        y_pred.append(pred)


    fpr, tpr, thresholds = metrics.roc_curve(y_true=y_true, y_score=y_pred)
    roc_auc = metrics.auc(fpr, tpr)
    return roc_auc

# def kl_divergence(m0, s0, m1, s1):
#     # convert to numpy, flatten
#     m0 = m0.numpy().flatten()
#     s0  = s0.numpy().flatten()
#     m1 = m1.numpy().flatten()[:m0.shape[0]]
#     s1 = s1.numpy().flatten()[:s0.shape[0]]
#     
#     # calc variance matrices from std
#     s0 = np.square(s0)
#     s1 = np.square(s1)
#     
#     # calc covariance matrices from variance
#     s0 = np.diag(s0)
#     s1 = np.diag(s1)
#     
#     # calc k
#     k = m0.shape[0]
#     
#     #calc parts of formula 
#     
#     # first part
#     s1_inverse = np.linalg.inv(s1)
#     s1_inverse_s0 = np.matmul(s1_inverse, s0)
#     trace = np.trace(s1_inverse_s0)
#     
#     # second part
#     m1_m0 = m1 - m0
#     mult = np.matmul(m1_m0, s1_inverse)
#     mult = np.matmul(mult, m1_m0)
#     
#     #fourth part
#     det_s0 = np.linalg.det(s0)
#     det_s1 = np.linalg.det(s1)
#     ln = np.log(det_s1/ det_s0)
#     
#     # calc kl divergence
#     kl = 0.5 * (trace + mult -k + ln)
#     
#     return kl

#%%

adj_snapshots = []
adj_norm_snapshots = []
features_snapshots = []
test_pos_edges_snapshot = []
test_edges_values_snapshot = []
test_neg_edges_snapshot = []
num_nodes_snapshot = []
limited_test_pos_edges_snapshot = []
limited_test_neg_edges_snapshot = []

hgraph = nx.Graph()
train_prev = None
for i in range(num_snapshots):
    clear_output()
    print(i)
    # adj_full_prev keeps a copy of previous step
    if i > 0:
        adj_full_prev = nx.adjacency_matrix(hgraph)
    
    # read edge list
    edgelist = load_edgelist(i)
    
    # add all edges to the graph, existing edges are not affected
    hgraph.add_weighted_edges_from(edgelist)
    
    # adj_full contains all edges
    adj_full = nx.adjacency_matrix(hgraph)
    
    # adj_inc contains new edges only
    if i > 0:
        adj_full_prev.resize(adj_full.shape)
        adj_inc = adj_full - adj_full_prev
    else:
        adj_inc = adj_full
    
    # get test edges; pos edges from adj_inc, neg edges from adj_full
    test_pos_edges, test_edges_values, test_neg_edges = sample_edges(adj_full, adj_inc, test_ratio)
    
    # get test edges that correspond to prev existing nodes
    node_limit = 0 if i == 0 else num_nodes_snapshot[i-1]
    limited_pos, limited_neg = sample_by_size(test_pos_edges, test_neg_edges, node_limit)
    
    # create the training adj, by
    # 1) removing the pos test edges from adj_inc
    # 2) adding the training adj from the previous step (if i>0)
    adj_test = matrix_from(test_pos_edges, test_edges_values, adj_inc.shape)
    adj_train = adj_inc - adj_test
    adj_train.eliminate_zeros()
    if i > 0:
        train_prev.resize(adj_train.shape)
        adj_train = adj_train + train_prev
    train_prev = adj_train
        
    # prepare adj tensor (dense)
    adj_train_with_diag = adj_train + sps.identity(adj_train.shape[0], dtype=np.float32).tocsr()
    adj_tensor = tf.Variable(adj_train_with_diag.todense(), dtype=tf.float32)

    # prepare adj normalized tensor (sparse)
    adj_norm = calc_normalized(adj_train_with_diag)
    indices = np.mat([adj_norm.row, adj_norm.col]).transpose()
    adj_norm_tensor = tf.SparseTensor(indices, adj_norm.data, adj_norm.shape)
    
    # create feature matrix (identity matrix)
    features = sps.identity(adj_norm.shape[0], dtype=np.float32, format='coo')
    
    # prepare feature tensor (sparse)
    indices = np.mat([features.row, features.col]).transpose()
    features_tensor = tf.SparseTensor(indices, features.data, features.shape)
    
    # append everything
    adj_snapshots.append(adj_tensor)
    adj_norm_snapshots.append(adj_norm_tensor)
    features_snapshots.append(features_tensor)
    test_pos_edges_snapshot.append(test_pos_edges)
    test_edges_values_snapshot.append(test_edges_values)
    test_neg_edges_snapshot.append(test_neg_edges)
    num_nodes_snapshot.append(adj_full.shape[0])
    limited_test_pos_edges_snapshot.append(limited_pos)
    limited_test_neg_edges_snapshot.append(limited_neg)

#%%

class FirstLayer(tf.keras.layers.Layer):
    def __init__(self, adj_norm, shared_w0):
        super(FirstLayer, self).__init__()
        self.adj_norm = adj_norm
        self.w = shared_w0

    def call(self, inputs, **kwargs):
        xw = tf.sparse.sparse_dense_matmul(inputs, self.w)
        axw = tf.sparse.sparse_dense_matmul(self.adj_norm, xw)
        relu = tf.nn.relu(axw)
        return relu
    
class SecondLayer(tf.keras.layers.Layer):

    def __init__(self, units, adj_norm):
        super(SecondLayer, self).__init__()
        self.units = units
        self.adj_norm = adj_norm
        self.training = True

    def build(self, input_shape):
        self.w = self.add_weight(shape=(input_shape[-1], self.units),
                                initializer=tf.keras.initializers.glorot_uniform(),
                                trainable=True)

    def call(self, inputs, **kwargs):
        x = tf.matmul(inputs, self.w)
        x = tf.sparse.sparse_dense_matmul(self.adj_norm, x)
        return x
    


class Encoder(tf.keras.Model):
    def __init__(self, adj_norm, embedding_size, shared_w0):
        super(Encoder, self).__init__()
        self.first_layer = FirstLayer(adj_norm, shared_w0)
        self.mean_layer = SecondLayer(embedding_size, adj_norm)
        self.std_layer = SecondLayer(embedding_size, adj_norm)
    
    def call(self, input_features, **kwargs):
        intermediate = self.first_layer(input_features)
        means = self.mean_layer(intermediate)
        stds = self.std_layer(intermediate)
        z = means + (tf.random.normal(shape=means.shape) * tf.exp(stds))
        return z, means, stds
    
class ThirdLayer(tf.keras.layers.Layer):

    def __init__(self):
        super(ThirdLayer, self).__init__()

    def call(self, inputs, **kwargs):
        matmul = tf.matmul(inputs, inputs, transpose_b=True)
        flat = tf.reshape(matmul, [-1])
        return flat

class Decoder(tf.keras.Model):
    def __init__(self):
        super(Decoder, self).__init__()
        self.third_layer = ThirdLayer()
    
    def call(self, input_features, **kwargs):
        return self.third_layer(input_features)
    
class Autoencoder(tf.keras.Model):
    def __init__(self, adj_norm, embedding_size, shared_w0):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder(adj_norm, embedding_size, shared_w0)
        self.decoder = Decoder()
        
    
    def call(self, input_features, **kwargs):
        z, means, stds = self.encoder(input_features)
        reconstructed = self.decoder(z)
        return reconstructed, means, stds    

#%%

opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
glorot_initializer = tf.keras.initializers.glorot_uniform()

autoencoders = []
pos_weights = []
norms = []
labels = []

for i in range(num_snapshots):
    adj = adj_snapshots[i]
    adj_norm = adj_norm_snapshots[i]
    adj_sum = tf.reduce_sum(adj)
    pos_weights.append(float((adj.shape[0] * adj.shape[0]) - adj_sum) / adj_sum)
    norms.append(adj.shape[0] * adj.shape[0] / float(((adj.shape[0] * adj.shape[0]) - adj_sum) * 2))
    labels.append(tf.reshape(adj, [-1]))

#%%

print("start training")
snapshot_history = defaultdict(list)
kl_loss_history = defaultdict(list)
reconstructed_loss_history = defaultdict(list)
testset_auc_history = []
future_auc_history = []

start_global = time.time()

kl_losses = {}
for i in range(num_snapshots):
    print('step', i)
    # predict new edges (existing nodes)
    if i > 0:
        last_trained_ae = autoencoders[i-1]
        last_features = features_snapshots[i-1]
        r, embeddings, s  = last_trained_ae(last_features)
        
        auc_score = auc(limited_test_pos_edges_snapshot[i], limited_test_neg_edges_snapshot[i], embeddings)
        future_auc_history.append(auc_score)
        print('future auc score', auc_score)
    else:
        future_auc_history.append(0)
    
    # prepare shared weights
    if i > 0:
        last_trained_ae = autoencoders[i-1]
        prev_w0 = last_trained_ae.encoder.first_layer.w
        num_new_nodes = num_nodes_snapshot[i] - num_nodes_snapshot[i-1]
        if num_new_nodes > 0:
            glorot_weights = tf.Variable(initial_value=glorot_initializer(shape=(num_new_nodes, intermediate_size), dtype=tf.float32), trainable=True)
            w0 = tf.concat([prev_w0, glorot_weights], axis=0)
        else:
            w0 = prev_w0
    else:
        w0 = tf.Variable(initial_value=glorot_initializer(shape=(num_nodes_snapshot[0], intermediate_size), dtype=tf.float32), trainable=True)
    
    # create autoencoder
    autoenc = Autoencoder(adj_norm_snapshots[i], embedding_size, w0)
    autoencoders.append(autoenc)
    features = features_snapshots[i]
    norm = norms[i]
    pos_weight = pos_weights[i]
    label = labels[i]
    num_nodes = num_nodes_snapshot[i]
    
    for epoch in range(epochs):
        start = time.time()
        with tf.GradientTape() as tape:
            reconstructed, means, stds = autoenc(features)
            reconstruction_loss = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=reconstructed, labels=label, pos_weight=pos_weight))
            kl_self_loss = tf.abs((0.5 / num_nodes_snapshot[i]) * tf.reduce_mean(tf.reduce_sum(1 + 2 * stds - tf.square(means) - tf.square(tf.exp(stds)), 1)))
            kl_loss = 0
            if i == 0:
                kl_loss += kl_self_loss
            else:
                for l in range(i-1, max(-1, i -1 - l_depth), -1):
                    prev_kl = kl_losses[l]
                    kl_loss += (kl_self_loss + prev_kl) / 2
            kl_losses[i] = kl_loss

            step_loss = reconstruction_loss + kl_loss
            snapshot_history[i].append(step_loss)
            kl_loss_history[i].append(kl_loss)
            reconstructed_loss_history[i].append(reconstruction_loss)
            
            # clear_output()
            if epoch % 20 == 0:
                print('epoch:', epoch, 'loss', step_loss.numpy(), 'exec time', time.time() - start)
            
        gradients = tape.gradient(step_loss, autoenc.trainable_variables)
        gradient_variables = zip(gradients, autoenc.trainable_variables)
        opt.apply_gradients(gradient_variables)
        
    reconstructed, embeddings, stds = autoenc(features)
    auc_score = auc(test_pos_edges_snapshot[i], test_neg_edges_snapshot[i], embeddings)
    testset_auc_history.append(auc_score)
    print('testset auc score', auc_score)
    
    
    
total = time.time() - start_global
print("elapsed: " + str(total) + " seconds")

#%%

%matplotlib qt

x_axis = range(epochs)
plt.figure()
for i in range(num_snapshots-1):
    plt.plot(x_axis, snapshot_history[i], label=str(i))
plt.xlabel('Epoch')
plt.ylabel('Total Loss per autoencoder')
plt.legend(loc="upper right")
plt.title('Total loss during training for each autoencoder')


plt.figure() 
for i in range(num_snapshots-1):
    plt.plot(x_axis, reconstructed_loss_history[i], label=str(i))
plt.xlabel('Epoch')
plt.ylabel('Reconstruction Loss per autoencoder')
plt.legend(loc="upper right")
plt.title('Reconstruction loss during training')

plt.figure()
for i in range(num_snapshots-1):
    plt.plot(x_axis, kl_loss_history[i], label=str(i))
plt.xlabel('Epoch')
plt.ylabel('KL Divergence Loss')
plt.legend(loc="upper right")
plt.title('KL Divergence during training')

x_axis = range(num_snapshots)
plt.figure()
plt.plot(x_axis, future_auc_history, label='VGAE trained on prev snapshot')
plt.plot(x_axis, testset_auc_history, label='VGAE trained on current snapshot')
plt.xlabel('Snapshot')
plt.ylabel('AUC')
plt.legend(loc="lower right")
plt.title("VGAE Performance for testsets")

plt.show()

#%%

